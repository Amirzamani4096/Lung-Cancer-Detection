{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1850f637-171b-4981-a4ec-302f73891b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "   Data = pickle.load(f)\n",
    "\n",
    "image_data = Data['images']\n",
    "bbox_list1 = Data['GT']\n",
    "offset_list_label_list1 = Data['reg']\n",
    "label_list1 = Data['clc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bddbc-3c9a-46f6-add3-ae24358efc7c",
   "metadata": {},
   "source": [
    "**Building RPN Using a Pretrained VGG16 Backbone:** This code constructs a Region Proposal Network (RPN) using a pretrained VGG16 model as the backbone for feature extraction. The code initializes a VGG16 network without the top classification layers and uses its convolutional layers to extract high-level feature maps from input images. The deconvolouthion of last layer of VGG16 are used to create a comprehensive feature representation. The features are then passed through additional convolutional layers to generate bounding box regression outputs (deltas) and objectiveness scores, which predict the presence of objects within the proposed regions. The pretrained weights from the original VGG16 model are transferred to the corresponding layers in the RPN model to leverage learned features, improving the efficiency of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ecdcdc-77d6-434e-94cb-4aa1e7e79c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\anaconda3\\envs\\vit-tf-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amir\\anaconda3\\envs\\vit-tf-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 800, 800]           1,792\n",
      "            Conv2d-2         [-1, 64, 800, 800]          36,928\n",
      "         MaxPool2d-3         [-1, 64, 400, 400]               0\n",
      "            Conv2d-4        [-1, 128, 400, 400]          73,856\n",
      "            Conv2d-5        [-1, 128, 400, 400]         147,584\n",
      "         MaxPool2d-6        [-1, 128, 200, 200]               0\n",
      "            Conv2d-7        [-1, 256, 200, 200]         295,168\n",
      "            Conv2d-8        [-1, 256, 200, 200]         590,080\n",
      "            Conv2d-9        [-1, 256, 200, 200]         590,080\n",
      "        MaxPool2d-10        [-1, 256, 100, 100]               0\n",
      "           Conv2d-11        [-1, 512, 100, 100]       1,180,160\n",
      "           Conv2d-12        [-1, 512, 100, 100]       2,359,808\n",
      "           Conv2d-13        [-1, 512, 100, 100]       2,359,808\n",
      "        MaxPool2d-14          [-1, 512, 50, 50]               0\n",
      "           Conv2d-15          [-1, 512, 50, 50]       2,359,808\n",
      "           Conv2d-16          [-1, 512, 50, 50]       2,359,808\n",
      "           Conv2d-17          [-1, 512, 50, 50]       2,359,808\n",
      "  ConvTranspose2d-18          [-1, 512, 98, 98]         262,656\n",
      "           Conv2d-19          [-1, 1, 100, 100]             513\n",
      "           Conv2d-20          [-1, 1, 100, 100]             257\n",
      "           Conv2d-21        [-1, 510, 100, 100]         261,630\n",
      "           Conv2d-22         [-1, 24, 100, 100]          12,312\n",
      "           Conv2d-23          [-1, 6, 100, 100]           3,078\n",
      "================================================================\n",
      "Total params: 15,255,134\n",
      "Trainable params: 15,255,134\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 7.32\n",
      "Forward/backward pass size (MB): 1543.71\n",
      "Params size (MB): 58.19\n",
      "Estimated Total Size (MB): 1609.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define the RPN model in PyTorch\n",
    "class CustomRPN(nn.Module):\n",
    "    def __init__(self, k=6, weight_decay=0.000001):\n",
    "        super(CustomRPN, self).__init__()\n",
    "\n",
    "        # Define the initial convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Transpose convolution to upsample feature maps\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=1, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # Convolutional layers to adjust channel dimensions\n",
    "        self.conv15 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, padding=1)\n",
    "        self.conv16 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n",
    "        self.conv17 = nn.Conv2d(in_channels=512, out_channels=510, kernel_size=1)\n",
    "        \n",
    "        # Define regressor and classifier layers\n",
    "        self.regressor = nn.Conv2d(in_channels=512, out_channels=4*k, kernel_size=1)\n",
    "        self.classifier = nn.Conv2d(in_channels=512, out_channels=k, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x1))\n",
    "        p1 = self.pool(x2)\n",
    "        x3 = F.relu(self.conv3(p1))\n",
    "        x4 = F.relu(self.conv4(x3))\n",
    "        p2 = self.pool(x4)\n",
    "        x5 = F.relu(self.conv5(p2))\n",
    "        x6 = F.relu(self.conv6(x5))\n",
    "        x7 = F.relu(self.conv7(x6))\n",
    "        p3 = self.pool(x7)\n",
    "        x8 = F.relu(self.conv8(p3))\n",
    "        x9 = F.relu(self.conv9(x8))\n",
    "        x10 = F.relu(self.conv10(x9))\n",
    "        p4 = self.pool(x10)\n",
    "        x11 = F.relu(self.conv11(p4))\n",
    "        x12 = F.relu(self.conv12(x11))\n",
    "        x13 = F.relu(self.conv13(x12))    \n",
    "        # Upsample the feature maps\n",
    "        x14 = self.deconv1(x13)\n",
    "        x15 = self.conv15(x14)\n",
    "        # Additional convolutions on earlier layers\n",
    "        x18 = self.conv16(p3)\n",
    "        x19 = self.conv17(x10)\n",
    "        # Concatenate feature maps along the channel dimension\n",
    "        concatenated = torch.cat([x15, x19, x18], dim=1)   \n",
    "        # Get bounding box predictions and objectness scores\n",
    "        regressor = self.regressor(concatenated)\n",
    "        classifier = torch.sigmoid(self.classifier(concatenated))\n",
    "        return regressor, classifier\n",
    "# Initialize the model\n",
    "model = CustomRPN(k=6)\n",
    "# Load pretrained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# Function to load weights from VGG16 to the CustomRPN model\n",
    "def load_vgg16_weights(custom_model, vgg16_model):\n",
    "    vgg16_layers = list(vgg16_model.features.children())  # Get layers from VGG16\n",
    "    custom_layers = list(custom_model.children())  # Get layers from CustomRPN\n",
    "    # Transfer weights from VGG16 to CustomRPN\n",
    "    vgg_idx = 0\n",
    "    for custom_layer in custom_layers:\n",
    "        if isinstance(custom_layer, nn.Conv2d):\n",
    "            vgg_layer = vgg16_layers[vgg_idx]\n",
    "            if isinstance(vgg_layer, nn.Conv2d):\n",
    "                with torch.no_grad():\n",
    "                    custom_layer.weight.copy_(vgg_layer.weight)\n",
    "                    custom_layer.bias.copy_(vgg_layer.bias)\n",
    "                vgg_idx += 1\n",
    "# Load the weights into the model\n",
    "load_vgg16_weights(model, vgg16)\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "summary(model, input_size=(3, 800, 800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b137d50-786b-4265-8350-c667208ac284",
   "metadata": {},
   "source": [
    "**Loss Functions:** In object detection models like the Region Proposal Network (RPN), training involves two primary tasks: bounding box regression and object classification. The smooth L1 loss is used for bounding box regression, which helps refine the anchor boxes to better match the ground truth boxes. The custom L1 loss function focuses on regressing anchor offsets, considering only the foreground (positive) examples. The custom binary loss function handles the classification task, calculating the binary cross-entropy loss for objectiveness scores, considering both positive and negative examples while ignoring neutral anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b4a095-bfe9-45bb-a1ac-7b9bc0c6a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmoothL1Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Calculate the absolute difference between true and predicted values\n",
    "        x = torch.abs(y_true - y_pred)\n",
    "        # Create a mask for values where the absolute difference is less than 1\n",
    "        mask = (x < 1.0).float()\n",
    "        # Compute the Smooth L1 loss based on the mask\n",
    "        loss = mask * (0.5 * x ** 2) + (1 - mask) * (x - 0.5)\n",
    "        return loss.mean()\n",
    "\n",
    "class CustomL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomL1Loss, self).__init__()\n",
    "        # Initialize SmoothL1Loss as a component of CustomL1Loss\n",
    "        self.smooth_l1_loss = SmoothL1Loss()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Reshape y_pred to match the format of y_true\n",
    "        y_pred = y_pred.view(y_true.shape[0], y_true.shape[1], -1)  \n",
    "        # Extract bounding box offsets and labels from y_true\n",
    "        offset_list = y_true[:, :, :-1]\n",
    "        label_list = y_true[:, :, -1]\n",
    "        # Identify positive examples where the label is 1\n",
    "        positive_idxs = (label_list == 1).nonzero(as_tuple=True)\n",
    "        # Select the predicted bounding boxes and target offsets for positive examples\n",
    "        bbox = y_pred[positive_idxs]\n",
    "        target_bbox = offset_list[positive_idxs]\n",
    "        # Compute the Smooth L1 loss between the predicted and target bounding boxes\n",
    "        loss = self.smooth_l1_loss(target_bbox, bbox)\n",
    "        return loss\n",
    "\n",
    "class CustomBinaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBinaryLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred_objectiveness):\n",
    "        # Reshape predictions to match the expected shape\n",
    "        y_pred = y_pred_objectiveness.view(-1, 60000)\n",
    "        y_true = y_true.squeeze(-1)\n",
    "        # Get the indices where the true labels are not -1\n",
    "        indices = (y_true != -1).nonzero(as_tuple=True)\n",
    "        # Select the predicted logits and true labels for the valid indices\n",
    "        rpn_match_logits = y_pred[indices]\n",
    "        anchor_class = y_true[indices]\n",
    "        # Clamp anchor_class values to be between 0 and 1\n",
    "        anchor_class = torch.clamp(anchor_class, min=0, max=1)   \n",
    "        loss = F.binary_cross_entropy(rpn_match_logits, anchor_class.float()) # Make sure anchor_class is float\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa391c-7e1b-4e28-a314-e982432eaeed",
   "metadata": {},
   "source": [
    "**Training and Saving Model:** In this step, we train the Region Proposal Network (RPN) model using 90% of the dataset as training data and 10% as validation data. After training, the model is saved for future evaluation. We use the Adam optimizer with a low learning rate to update the model weights during training. The loss functions for bounding box regression and object classification are defined earlier, and they are used to compute the total loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a459188-b53e-4e82-8a35-e887327d45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Transpose the image data to match the PyTorch tensor format (N, C, H, W)\n",
    "image_data1 = np.transpose(image_data, (0, 3, 1, 2))\n",
    "\n",
    "# Convert image data and target labels to PyTorch tensors\n",
    "images = torch.tensor(image_data1)\n",
    "images = images.to(dtype=torch.float32)\n",
    "target_regressor = torch.tensor(offset_list_label_list1)\n",
    "target_classifier = torch.tensor(label_list1)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_images, val_images, train_target_regressor, val_target_regressor, train_target_classifier, val_target_classifier = train_test_split(\n",
    "    images, target_regressor, target_classifier, test_size=0.1)\n",
    "\n",
    "# Create DataLoader for training data with a batch size of 4\n",
    "train_dataset = TensorDataset(train_images, train_target_regressor, train_target_classifier)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "# Create DataLoader for validation data with a batch size of 4\n",
    "val_dataset = TensorDataset(val_images, val_target_regressor, val_target_classifier)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise CPU\n",
    "model = CustomRPN(k=6).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer with learning rate of 0.0001\n",
    "\n",
    "# Define the loss functions\n",
    "custom_l1_loss = CustomL1Loss()\n",
    "custom_binary_loss = CustomBinaryLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images, target_regressor, target_classifier = batch\n",
    "        images, target_regressor, target_classifier = images.to(device), target_regressor.to(device), target_classifier.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        regressor, classifier = model(images)\n",
    "\n",
    "        # Compute losses for regression and classification\n",
    "        loss_regressor = custom_l1_loss(target_regressor, regressor)\n",
    "        loss_classifier = custom_binary_loss(target_classifier, classifier)\n",
    "        loss = loss_classifier + 0.2 * loss_regressor  # Combine losses with a weight for regression loss\n",
    "\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)  # Compute average loss\n",
    "    print(f'Average Loss: {average_loss}')\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No gradient computation for validation\n",
    "        for batch in dataloader:\n",
    "            images, target_regressor, target_classifier = batch\n",
    "            images, target_regressor, target_classifier = images.to(device), target_regressor.to(device), target_classifier.to(device)\n",
    "            \n",
    "            regressor, classifier = model(images)\n",
    "\n",
    "            # Compute losses for regression and classification\n",
    "            loss_regressor = custom_l1_loss(target_regressor, regressor)\n",
    "            loss_classifier = custom_binary_loss(target_classifier, classifier)\n",
    "            loss = loss_classifier + 0.2 * loss_regressor  # Combine losses with a weight for regression loss\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)  # Compute average loss\n",
    "    print(f'Validation Loss: {average_loss}')\n",
    "\n",
    "# Set the number of epochs and batch size\n",
    "num_epochs = 1000  # Number of epochs for training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, train_dataloader, optimizer, device)  # Train for one epoch\n",
    "    validate(model, val_dataloader, device)  # Validate after training\n",
    "    print('-' * 30)\n",
    "\n",
    "# Save the trained model to a file\n",
    "torch.save(model, 'RPN_torch.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
